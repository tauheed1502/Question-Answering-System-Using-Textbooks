# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1li--xEZJ6ZFdZkyQO6w25S7gpovHMGhK
"""

pip install PyPDF2 pdfminer.six

from PyPDF2 import PdfReader

# Example of extracting text from a PDF (Using PyPDF2)
from PyPDF2 import PdfReader

def extract_text_from_pdf(pdf_path):
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    return text


# Example Usage
pdf_path = "/content/Environmental Science Earth as a Living Planet by Daniel B. Botkin and Edward A. Keller.pdf"  # Replace with your file
extracted_textE = extract_text_from_pdf(pdf_path)
pdf_path = "/content/Data Structures and Algorithms in Java Fourth Edition.pdf"  # Replace with your file
extracted_textD = extract_text_from_pdf(pdf_path)
pdf_path = "/content/best book Principles of Artificial Intelligence ( PDFDrive ).pdf"  # Replace with your file
extracted_textB = extract_text_from_pdf(pdf_path)

# Save extracted text to a file
with open("textbookE.txt", "w", encoding="utf-8") as f:
    f.write(extracted_textE)
with open("textbookD.txt", "w", encoding="utf-8") as f:
    f.write(extracted_textD)
with open("textbookB.txt", "w", encoding="utf-8") as f:
    f.write(extracted_textB)
print("Text extraction complete!")

import re
import json

def organize_textbook(file_path, textbook_title):
    """
    Organizes a textbook's text into chapters and sections.

    Args:
        file_path (str): Path to the .txt file containing textbook content.
        textbook_title (str): Title of the textbook for reference.

    Returns:
        dict: Hierarchical structure with chapters, sections, and paragraphs.
    """
    # Read the file content
    with open(file_path, "r", encoding="utf-8") as file:
        text = file.read()

    # Split text into chapters using a regex pattern for "Chapter X"
    chapters = re.split(r"\nChapter\s+\d+[:\s]", text, flags=re.IGNORECASE)
    hierarchy = {"textbook_title": textbook_title, "chapters": []}

    # Process each chapter
    for chapter_num, chapter_text in enumerate(chapters):
        if chapter_text.strip():  # Ignore empty splits
            chapter_data = {
                "chapter_number": chapter_num + 1,
                "sections": []
            }

            # Split chapter into sections using "Section X.Y"
            sections = re.split(r"\nSection\s+\d+(\.\d+)*[:\s]", chapter_text, flags=re.IGNORECASE)

            for section_num, section_text in enumerate(sections):
                if section_text.strip():  # Ignore empty splits
                    section_data = {
                        "section_number": section_num + 1,
                        "content": [p.strip() for p in section_text.split("\n\n") if p.strip()]
                    }
                    chapter_data["sections"].append(section_data)

            hierarchy["chapters"].append(chapter_data)

    return hierarchy

# Process all three books
books = [
    {"file": "textbookE.txt", "title": "Book E"},
    {"file": "textbookB.txt", "title": "Book B"},
    {"file": "textbookD.txt", "title": "Book D"}
]

organized_books = []

for book in books:
    organized_books.append(organize_textbook(book["file"], book["title"]))

# Save the organized structure for each book as JSON
for book, data in zip(books, organized_books):
    output_file = f"{book['title'].replace(' ', '_').lower()}_organized.json"
    with open(output_file, "w", encoding="utf-8") as json_file:
        json.dump(data, json_file, indent=4)

print("Books organized and saved as JSON!")

import re
import json
import networkx as nx

def sanitize_text(text):
    """
    Sanitizes text to ensure it is XML-compatible.
    Removes control characters and ensures UTF-8 compliance.

    Args:
        text (str): Text to sanitize.

    Returns:
        str: Sanitized text.
    """
    if not text:
        return ""
    # Remove control characters and ensure UTF-8 encoding
    text = re.sub(r"[\x00-\x1F\x7F]", "", text)  # Remove non-printable characters
    text = text.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    return text

def build_hierarchical_tree(hierarchy):
    """
    Builds a hierarchical tree (graph) from the organized textbook data.

    Args:
        hierarchy (dict): Organized textbook data with chapters, sections, and paragraphs.

    Returns:
        nx.DiGraph: A directed graph representing the hierarchical tree.
    """
    graph = nx.DiGraph()
    textbook_title = hierarchy["textbook_title"]
    graph.add_node(textbook_title, node_type="root")

    # Add chapters and sections as nodes
    for chapter in hierarchy["chapters"]:
        chapter_id = f"Chapter {chapter['chapter_number']}"
        graph.add_node(chapter_id, node_type="chapter")
        graph.add_edge(textbook_title, chapter_id)  # Connect chapter to root

        for section in chapter["sections"]:
            section_id = f"{chapter_id} - Section {section['section_number']}"
            graph.add_node(section_id, node_type="section")
            graph.add_edge(chapter_id, section_id)  # Connect section to chapter

            # Add paragraphs as leaf nodes
            for i, paragraph in enumerate(section["content"], 1):
                sanitized_paragraph = sanitize_text(paragraph)
                paragraph_id = f"{section_id} - Paragraph {i}"
                graph.add_node(paragraph_id, node_type="paragraph", content=sanitized_paragraph)
                graph.add_edge(section_id, paragraph_id)  # Connect paragraph to section

    return graph

# Process all three books and save hierarchical trees
organized_files = [
    {"json_file": "book_e_organized.json", "title": "Book E"},
    {"json_file": "book_b_organized.json", "title": "Book B"},
    {"json_file": "book_d_organized.json", "title": "Book D"}
]

for book in organized_files:
    # Load the organized JSON
    with open(book["json_file"], "r", encoding="utf-8") as json_file:
        organized_hierarchy = json.load(json_file)

    # Build the hierarchical tree
    tree = build_hierarchical_tree(organized_hierarchy)

    # Save the tree as a GraphML file
    output_file = f"{book['title'].replace(' ', '_').lower()}_tree.graphml"
    nx.write_graphml(tree, output_file)
    print(f"Hierarchical tree for {book['title']} saved as {output_file}")

!pip install nltk
!pip install rank-bm25
!pip install sentence-transformers

import nltk
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize

# Download required NLTK data
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')  # WordNet for multilingual support

def expand_query(query):
    """
    Expands a user query by adding synonyms for each word in the query.

    Args:
        query (str): User query.

    Returns:
        list: Expanded list of query terms.
    """
    expanded_terms = set()
    tokens = word_tokenize(query)

    for token in tokens:
        expanded_terms.add(token)  # Add the original word

        # Add synonyms using WordNet
        for syn in wordnet.synsets(token):
            for lemma in syn.lemmas():
                expanded_terms.add(lemma.name().replace('_', ' '))  # Add synonym

    return list(expanded_terms)

# Example usage
user_query = "machine learning techniques"
expanded_query = expand_query(user_query)
print("Expanded Query Terms:", expanded_query)

from rank_bm25 import BM25Okapi
from nltk.tokenize import word_tokenize

# Example paragraphs for retrieval (You should replace this with actual data from the textbooks)
all_paragraphs = [
    {"id": 1, "content": "Machine learning is a branch of artificial intelligence."},
    {"id": 2, "content": "Deep learning is a subfield of machine learning."},
    {"id": 3, "content": "Natural language processing involves machine learning techniques."}
]

# Tokenize paragraphs
tokenized_corpus = [word_tokenize(paragraph['content'].lower()) for paragraph in all_paragraphs]
bm25 = BM25Okapi(tokenized_corpus)

def bm25_retrieve(query, top_k=5):
    """
    Retrieves top-k paragraphs using BM25.

    Args:
        query (str): User query.
        top_k (int): Number of top results to return.

    Returns:
        list: Top-k matching paragraphs.
    """
    tokenized_query = word_tokenize(query.lower())
    scores = bm25.get_scores(tokenized_query)
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
    return [all_paragraphs[i] for i in top_indices]

# Example usage
query = "neural networks"
bm25_results = bm25_retrieve(query)
print("BM25 Results:", bm25_results)

def dense_retrieve(query, top_k=5):
    """
    Retrieves top-k paragraphs using Sentence-BERT for dense retrieval.

    Args:
        query (str): User query.
        top_k (int): Number of top results to return.

    Returns:
        list: Top-k matching paragraphs.
    """
    query_embedding = model.encode(query, convert_to_tensor=True)
    scores = util.cos_sim(query_embedding, paragraph_embeddings)[0]

    # Ensure top_k doesn't exceed the number of available paragraphs
    top_k = min(top_k, len(all_paragraphs))

    top_indices = scores.topk(k=top_k).indices
    return [all_paragraphs[i] for i in top_indices]

# Example usage
query = "deep learning applications"
dense_results = dense_retrieve(query)
print("Dense Results:", dense_results)

def hybrid_retrieve(query, bm25_weight=0.5, top_k=5):
    """
    Combines BM25 and Sentence-BERT results for hybrid retrieval.

    Args:
        query (str): User query.
        bm25_weight (float): Weight for BM25 scores in the hybrid method.
        top_k (int): Number of top results to return.

    Returns:
        list: Top-k matching paragraphs, re-ranked by hybrid scores.
    """
    # BM25 scores
    tokenized_query = word_tokenize(query.lower())
    bm25_scores = bm25.get_scores(tokenized_query)

    # Dense retrieval scores
    query_embedding = model.encode(query, convert_to_tensor=True)
    dense_scores = util.cos_sim(query_embedding, paragraph_embeddings)[0].cpu().numpy()

    # Hybrid scores
    hybrid_scores = bm25_weight * bm25_scores + (1 - bm25_weight) * dense_scores
    top_indices = sorted(range(len(hybrid_scores)), key=lambda i: hybrid_scores[i], reverse=True)[:top_k]
    return [all_paragraphs[i] for i in top_indices]

# Example usage
query = "neural networks and backpropagation"
hybrid_results = hybrid_retrieve(query)
print("Hybrid Results:", hybrid_results)

#Step4

!pip install openai

!pip install --upgrade openai

import os
import json

def preprocess_text(book_file, output_file):
    """Preprocess a text file into a hierarchical structure and save as JSON."""
    with open(book_file, 'r') as f:
        content = f.read()

    paragraphs = content.split("\n\n")  # Split into paragraphs
    hierarchical_tree = {"paragraphs": paragraphs}  # Simple structure, could be expanded

    with open(output_file, 'w') as f:
        json.dump(hierarchical_tree, f)

# Process your books
preprocess_text("book_e.txt", "book_e_tree.json")
preprocess_text("book_b.txt", "book_b_tree.json")
preprocess_text("book_d.txt", "book_d_tree.json")

from sentence_transformers import SentenceTransformer, util
import json

# Load model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

def retrieve_relevant_content(query, top_k=3):
    """Retrieve relevant paragraphs for the query."""
    paragraphs = []
    # Load the hierarchical trees
    for book_file in ["book_e_tree.json", "book_b_tree.json", "book_d_tree.json"]:
        with open(book_file, "r") as f:
            hierarchical_tree = json.load(f)
            paragraphs.extend(hierarchical_tree["paragraphs"])

    # Create embeddings for the paragraphs
    paragraph_embeddings = model.encode(paragraphs, convert_to_tensor=True)

    # Create query embedding
    query_embedding = model.encode(query, convert_to_tensor=True)

    # Calculate similarity scores
    scores = util.cos_sim(query_embedding, paragraph_embeddings)[0]

    # Get top-k relevant paragraphs based on similarity score
    top_k_indices = scores.topk(k=top_k).indices
    relevant_paragraphs = [paragraphs[i] for i in top_k_indices]

    return relevant_paragraphs

# Example usage:
query = "What are the applications of deep learning?"
retrieved_content = retrieve_relevant_content(query)
print("Retrieved Content:", retrieved_content)

#Step4.3

pip install transformers torch

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load pre-trained GPT-2 model and tokenizer
model_name = "gpt2"  # You can switch to "gpt2-medium", "gpt2-large", or "gpt2-xl" for more power
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

def generate_answer_hf(query, retrieved_content):
    """
    Generate an answer using Hugging Face GPT-2 model.

    Parameters:
        query (str): The user's question.
        retrieved_content (str): The relevant content retrieved from the database.

    Returns:
        str: The generated answer.
    """
    # Prepare the input prompt
    prompt = f"Given the following context, answer the query.\n\nContext:\n{retrieved_content}\n\nQuery: {query}\n\nAnswer:"

    # Tokenize input
    inputs = tokenizer.encode(prompt, return_tensors="pt", max_length=1024, truncation=True)

    # Generate a response from the model
    output = model.generate(inputs, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2, top_p=0.95, temperature=0.7)

    # Decode and return the answer
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return answer

# Example usage
query = "What are the applications of deep learning?"
retrieved_content = "Deep learning is widely used in computer vision, natural language processing, and robotics."
answer = generate_answer_hf(query, retrieved_content)
print("Generated Answer:", answer)

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
import json

# Load pre-trained GPT-2 model and tokenizer
model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Function to generate answer using GPT-2
def generate_answer_hf(query, retrieved_content):
    prompt = f"Given the following context, answer the query.\n\nContext:\n{retrieved_content}\n\nQuery: {query}\n\nAnswer:"
    inputs = tokenizer.encode(prompt, return_tensors="pt", max_length=1024, truncation=True)
    output = model.generate(inputs, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2, top_p=0.95, temperature=0.7)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return answer

# Simulate retrieving content (replace with your actual retrieval logic)
retrieved_content = "Deep learning is a subset of machine learning in which algorithms learn from large amounts of data."
query = "What is deep learning?"

# Generate the answer
answer = generate_answer_hf(query, retrieved_content)
print("Generated Answer:", answer)

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
import json

# Load pre-trained GPT-2 model and tokenizer
model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Function to generate answer using GPT-2
def generate_answer_hf(query, retrieved_content):
    prompt = f"Given the following context, answer the query.\n\nContext:\n{retrieved_content}\n\nQuery: {query}\n\nAnswer:"
    inputs = tokenizer.encode(prompt, return_tensors="pt", max_length=1024, truncation=True)
    output = model.generate(inputs, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2, top_p=0.95, temperature=0.7)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return answer

# Simulate retrieving content (replace with your actual retrieval logic)
retrieved_content = "Deep learning is a subset of machine learning in which algorithms learn from large amounts of data."
query = "What is deep learning?"

# Generate the answer
answer = generate_answer_hf(query, retrieved_content)
print("Generated Answer:", answer)

pip install nltk rouge-score

from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer

# Define reference and generated answer (for testing)
reference = ["Deep learning is a subset of machine learning that teaches computers to learn from large amounts of data."]
generated_answer = "Deep learning refers to the process of learning from a large set of information."

# BLEU score
bleu_score = sentence_bleu([reference], generated_answer.split())
print(f"BLEU score: {bleu_score}")

# ROUGE score
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
scores = scorer.score(' '.join(reference), generated_answer)
print(f"ROUGE score: {scores}")

queries = [
    "What is machine learning?",
    "Explain the applications of deep learning in healthcare.",
    "What is the difference between supervised and unsupervised learning?",
    "What are the limitations of neural networks?"
]

for query in queries:
    answer = generate_answer_hf(query, retrieved_content)
    print(f"Query: {query}")
    print(f"Answer: {answer}\n")

